{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1r3071bFUFK"
      },
      "outputs": [],
      "source": [
        "# config.py\n",
        "\n",
        "class Config(object):\n",
        "    embed_size = 300\n",
        "    hidden_layers = 1\n",
        "    hidden_size = 64\n",
        "    output_size = 4\n",
        "    max_epochs = 2\n",
        "    hidden_size_linear = 64\n",
        "    lr = 0.5\n",
        "    batch_size = 128\n",
        "    seq_len = None # Sequence length for RNN\n",
        "    dropout_keep = 0.8\n",
        "    max_sen_len = 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python==3.5.0\n",
        "!pip install pandas==0.23.4\n",
        "!pip install numpy==1.15.2\n",
        "!pip install spacy==2.0.13\n",
        "!pip install torch==0.4.1.post2\n",
        "!pip install torchtext==0.3.1\n"
      ],
      "metadata": {
        "id": "mqsh9S2Rs42Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.py\n",
        "\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext.vocab import Vectors\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchtext import data\n",
        "\n",
        "\n",
        "class Dataset(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.train_iterator = None\n",
        "        self.test_iterator = None\n",
        "        self.val_iterator = None\n",
        "        self.vocab = []\n",
        "        self.word_embeddings = {}\n",
        "\n",
        "    def parse_label(self, label):\n",
        "        '''\n",
        "        Get the actual labels from label string\n",
        "        Input:\n",
        "            label (string) : labels of the form '__label__2'\n",
        "        Returns:\n",
        "            label (int) : integer value corresponding to label string\n",
        "        '''\n",
        "        return int(label.strip()[-1])\n",
        "\n",
        "    def get_pandas_df(self, filename):\n",
        "        '''\n",
        "        Load the data into Pandas.DataFrame object\n",
        "        This will be used to convert data to torchtext object\n",
        "        '''\n",
        "        with open(filename, 'r') as datafile:\n",
        "            data = [line.strip().split(',', maxsplit=1) for line in datafile]\n",
        "            data_text = list(map(lambda x: x[1], data))\n",
        "            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n",
        "\n",
        "        full_df = pd.DataFrame({\"text\":data_text, \"label\":data_label})\n",
        "        return full_df\n",
        "\n",
        "    def load_data(self, w2v_file, train_file, test_file, val_file=None):\n",
        "        '''\n",
        "        Loads the data from files\n",
        "        Sets up iterators for training, validation and test data\n",
        "        Also create vocabulary and word embeddings based on the data\n",
        "\n",
        "        Inputs:\n",
        "            w2v_file (String): absolute path to file containing word embeddings (GloVe/Word2Vec)\n",
        "            train_file (String): absolute path to training file\n",
        "            test_file (String): absolute path to test file\n",
        "            val_file (String): absolute path to validation file\n",
        "        '''\n",
        "\n",
        "        NLP = spacy.load('en_core_web_sm')\n",
        "        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != \" \"]\n",
        "\n",
        "        # Creating Field for data\n",
        "        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n",
        "        LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "        datafields = [(\"text\",TEXT),(\"label\",LABEL)]\n",
        "\n",
        "        # Load data from pd.DataFrame into torchtext.data.Dataset\n",
        "        train_df = self.get_pandas_df(train_file)\n",
        "        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n",
        "        train_data = data.Dataset(train_examples, datafields)\n",
        "\n",
        "        test_df = self.get_pandas_df(test_file)\n",
        "        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n",
        "        test_data = data.Dataset(test_examples, datafields)\n",
        "\n",
        "        # If validation file exists, load it. Otherwise get validation data from training data\n",
        "        if val_file:\n",
        "            val_df = self.get_pandas_df(val_file)\n",
        "            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n",
        "            val_data = data.Dataset(val_examples, datafields)\n",
        "        else:\n",
        "            train_data, val_data = train_data.split(split_ratio=0.8)\n",
        "\n",
        "        TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n",
        "        self.word_embeddings = TEXT.vocab.vectors\n",
        "        self.vocab = TEXT.vocab\n",
        "\n",
        "        self.train_iterator = data.BucketIterator(\n",
        "            (train_data),\n",
        "            batch_size=self.config.batch_size,\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            repeat=False,\n",
        "            shuffle=True)\n",
        "\n",
        "        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n",
        "            (val_data, test_data),\n",
        "            batch_size=self.config.batch_size,\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            repeat=False,\n",
        "            shuffle=False)\n",
        "\n",
        "        print (\"Loaded {} training examples\".format(len(train_data)))\n",
        "        print (\"Loaded {} test examples\".format(len(test_data)))\n",
        "        print (\"Loaded {} validation examples\".format(len(val_data)))\n",
        "\n",
        "\n",
        "def evaluate_model(model, iterator):\n",
        "    all_preds = []\n",
        "    all_y = []\n",
        "    for idx,batch in enumerate(iterator):\n",
        "        if torch.cuda.is_available():\n",
        "            x = batch.text.cuda()\n",
        "        else:\n",
        "            x = batch.text\n",
        "        y_pred = model(x)\n",
        "        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n",
        "        all_preds.extend(predicted.numpy())\n",
        "        all_y.extend(batch.label.numpy())\n",
        "    score = accuracy_score(all_y, np.array(all_preds).flatten())\n",
        "    return score"
      ],
      "metadata": {
        "id": "C4jBww4BFqUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class RCNN(nn.Module):\n",
        "    def __init__(self, config, vocab_size, word_embeddings):\n",
        "        super(RCNN, self).__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
        "        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
        "\n",
        "        # Bi-directional LSTM for RCNN\n",
        "        self.lstm = nn.LSTM(input_size = self.config.embed_size,\n",
        "                            hidden_size = self.config.hidden_size,\n",
        "                            num_layers = self.config.hidden_layers,\n",
        "                            dropout = self.config.dropout_keep,\n",
        "                            bidirectional = True)\n",
        "\n",
        "        self.dropout = nn.Dropout(self.config.dropout_keep)\n",
        "\n",
        "        # Linear layer to get \"convolution output\" to be passed to Pooling Layer\n",
        "        self.W = nn.Linear(\n",
        "            self.config.embed_size + 2*self.config.hidden_size,\n",
        "            self.config.hidden_size_linear\n",
        "        )\n",
        "\n",
        "        # Tanh non-linearity\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        # Fully-Connected Layer\n",
        "        self.fc = nn.Linear(\n",
        "            self.config.hidden_size_linear,\n",
        "            self.config.output_size\n",
        "        )\n",
        "\n",
        "        # Softmax non-linearity\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (seq_len, batch_size)\n",
        "        embedded_sent = self.embeddings(x)\n",
        "        # embedded_sent.shape = (seq_len, batch_size, embed_size)\n",
        "\n",
        "        lstm_out, (h_n,c_n) = self.lstm(embedded_sent)\n",
        "        # lstm_out.shape = (seq_len, batch_size, 2 * hidden_size)\n",
        "\n",
        "        input_features = torch.cat([lstm_out,embedded_sent], 2).permute(1,0,2)\n",
        "        # final_features.shape = (batch_size, seq_len, embed_size + 2*hidden_size)\n",
        "\n",
        "        linear_output = self.tanh(\n",
        "            self.W(input_features)\n",
        "        )\n",
        "        # linear_output.shape = (batch_size, seq_len, hidden_size_linear)\n",
        "\n",
        "        linear_output = linear_output.permute(0,2,1) # Reshaping fot max_pool\n",
        "\n",
        "        max_out_features = F.max_pool1d(linear_output, linear_output.shape[2]).squeeze(2)\n",
        "        # max_out_features.shape = (batch_size, hidden_size_linear)\n",
        "\n",
        "        max_out_features = self.dropout(max_out_features)\n",
        "        final_out = self.fc(max_out_features)\n",
        "        return self.softmax(final_out)\n",
        "\n",
        "    def add_optimizer(self, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def add_loss_op(self, loss_op):\n",
        "        self.loss_op = loss_op\n",
        "\n",
        "    def reduce_lr(self):\n",
        "        print(\"Reducing LR\")\n",
        "        for g in self.optimizer.param_groups:\n",
        "            g['lr'] = g['lr'] / 2\n",
        "\n",
        "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
        "        train_losses = []\n",
        "        val_accuracies = []\n",
        "        losses = []\n",
        "\n",
        "        # Reduce learning rate as number of epochs increase\n",
        "        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n",
        "            self.reduce_lr()\n",
        "\n",
        "        for i, batch in enumerate(train_iterator):\n",
        "            self.optimizer.zero_grad()\n",
        "            if torch.cuda.is_available():\n",
        "                x = batch.text.cuda()\n",
        "                y = (batch.label - 1).type(torch.cuda.LongTensor)\n",
        "            else:\n",
        "                x = batch.text\n",
        "                y = (batch.label - 1).type(torch.LongTensor)\n",
        "            y_pred = self.__call__(x)\n",
        "            loss = self.loss_op(y_pred, y)\n",
        "            loss.backward()\n",
        "            losses.append(loss.data.cpu().numpy())\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(\"Iter: {}\".format(i+1))\n",
        "                avg_train_loss = np.mean(losses)\n",
        "                train_losses.append(avg_train_loss)\n",
        "                print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
        "                losses = []\n",
        "\n",
        "                # Evalute Accuracy on validation set\n",
        "                val_accuracy = evaluate_model(self, val_iterator)\n",
        "                print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
        "                self.train()\n",
        "\n",
        "        return train_losses, val_accuracies"
      ],
      "metadata": {
        "id": "lgHzdCPJFuL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iGN5vOyIJM-",
        "outputId": "48bc698f-3369-4e75-98e0-4b7ca89b93a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMlxresHINnJ",
        "outputId": "1ec75b2d-63f0-463a-de97-6e53c0643c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-03 08:20:30.618412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.cli.download import download\n",
        "download(model=\"en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP_senMKa2uZ",
        "outputId": "ed466190-5929-4ebd-fd8a-25faec6de060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1H4JNrnptBs",
        "outputId": "c64400d8-0c00-4f20-e97f-5e119197a24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-03 10:47:23--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2023-09-03 10:47:23--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2023-09-03 10:47:24--  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  4.99MB/s    in 6m 50s  \n",
            "\n",
            "2023-09-03 10:54:14 (5.07 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C95rUb9p99y",
        "outputId": "431c72e3-6176-402c-9b4e-fad6cb2fe8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI_i5yhPqAcU",
        "outputId": "8a11b7e3-d2d2-4fb3-c7a3-54cb364ace47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ag_news.test   glove.840B.300d.txt  processed_dev.txt   processed_train.txt\n",
            "ag_news.train  glove.840B.300d.zip  processed_test.txt  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchtext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhSxSAlPsIBX",
        "outputId": "06a4a649-8a85-4923-b870-250c344cc1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (2.0.4)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import sys\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import torch\n",
        "from torchtext import data\n",
        "if __name__=='__main__':\n",
        "    config = Config()\n",
        "    train_file = 'ag_news.train'\n",
        "    test_file = 'ag_news.test'\n",
        "    w2v_file = 'glove.840B.300d.txt'\n",
        "\n",
        "    dataset = Dataset(config)\n",
        "    dataset.load_data(w2v_file, train_file, test_file)\n",
        "\n",
        "    # Create Model with specified optimizer and loss function\n",
        "    ##############################################################\n",
        "    model = RCNN(config, len(dataset.vocab), dataset.word_embeddings)\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    model.train()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config.lr)\n",
        "    NLLLoss = nn.NLLLoss()\n",
        "    model.add_optimizer(optimizer)\n",
        "    model.add_loss_op(NLLLoss)\n",
        "    ##############################################################\n",
        "\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for i in range(config.max_epochs):\n",
        "        print (\"Epoch: {}\".format(i))\n",
        "        train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
        "        train_losses.append(train_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "    train_acc = evaluate_model(model, dataset.train_iterator)\n",
        "    val_acc = evaluate_model(model, dataset.val_iterator)\n",
        "    test_acc = evaluate_model(model, dataset.test_iterator)\n",
        "\n",
        "    print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
        "    print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n",
        "    print ('Final Test Accuracy: {:.4f}'.format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmQ5zMpWF3Ub",
        "outputId": "a0e23902-8dce-43c9-9d2d-dfea5dd8307b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 96000 training examples\n",
            "Loaded 7600 test examples\n",
            "Loaded 24000 validation examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Reducing LR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 1\n",
            "\tAverage training loss: -0.24976\n",
            "\tVal Accuracy: 0.2545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 101\n",
            "\tAverage training loss: -0.36984\n",
            "\tVal Accuracy: 0.6299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 201\n",
            "\tAverage training loss: -0.62754\n",
            "\tVal Accuracy: 0.7825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 301\n",
            "\tAverage training loss: -0.74235\n",
            "\tVal Accuracy: 0.8211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 401\n",
            "\tAverage training loss: -0.78350\n",
            "\tVal Accuracy: 0.8352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 501\n",
            "\tAverage training loss: -0.80834\n",
            "\tVal Accuracy: 0.8413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 601\n",
            "\tAverage training loss: -0.81284\n",
            "\tVal Accuracy: 0.8430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 701\n",
            "\tAverage training loss: -0.82088\n",
            "\tVal Accuracy: 0.8495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Reducing LR\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.87290\n",
            "\tVal Accuracy: 0.8498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 101\n",
            "\tAverage training loss: -0.82857\n",
            "\tVal Accuracy: 0.8508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 201\n",
            "\tAverage training loss: -0.83399\n",
            "\tVal Accuracy: 0.8530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 301\n",
            "\tAverage training loss: -0.83614\n",
            "\tVal Accuracy: 0.8525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 401\n",
            "\tAverage training loss: -0.83614\n",
            "\tVal Accuracy: 0.8580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 501\n",
            "\tAverage training loss: -0.84031\n",
            "\tVal Accuracy: 0.8567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 601\n",
            "\tAverage training loss: -0.83659\n",
            "\tVal Accuracy: 0.8555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 701\n",
            "\tAverage training loss: -0.83677\n",
            "\tVal Accuracy: 0.8566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n",
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n",
            "<ipython-input-32-9bef7561cd76>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Accuracy: 0.8569\n",
            "Final Validation Accuracy: 0.8611\n",
            "Final Test Accuracy: 0.8614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.3.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "2vPTzw37F9Nj",
        "outputId": "57ac919a-6dde-4088-efed-193a57c13afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.3.1\n",
            "  Downloading torchtext-0.3.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m920.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.3.1) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.3.1) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.3.1) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.3.1) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.3.1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.3.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.3.1) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.3.1) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.3.1) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.3.1) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.3.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.3.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.3.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.3.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.3.1) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.3.1) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.3.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.3.1) (1.3.0)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "Successfully installed torchtext-0.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6 torch==1.5"
      ],
      "metadata": {
        "id": "_vU1HyMZx0Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual inspection section\n",
        "while True:\n",
        "    custom_text = input(\"Enter a text to classify (or 'exit' to quit): \")\n",
        "    if custom_text.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Tokenize and preprocess the custom text using the TEXT field\n",
        "    custom_text = [token.text for token in nlp(custom_text)]\n",
        "\n",
        "    # Convert tokens to indices using the model's vocabulary\n",
        "    custom_text_indices = [dataset.vocab.stoi[token] for token in custom_text]\n",
        "\n",
        "    # Convert to a PyTorch tensor and make a prediction\n",
        "    with torch.no_grad():\n",
        "        if torch.cuda.is_available():\n",
        "            custom_text_tensor = torch.tensor(custom_text_indices, dtype=torch.long).cuda()\n",
        "        else:\n",
        "            custom_text_tensor = torch.tensor(custom_text_indices, dtype=torch.long)\n",
        "        predicted_scores = model(custom_text_tensor.unsqueeze(0))\n",
        "        predicted_label = torch.argmax(predicted_scores, dim=1).item()\n",
        "\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "vJcc2kYBxZED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual inspection section\n",
        "while True:\n",
        "    custom_text = input(\"Enter a text to classify (or 'exit' to quit): \")\n",
        "    if custom_text.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Tokenize and preprocess the custom text using the TEXT field\n",
        "    custom_text = [token.text for token in nlp(custom_text)]\n",
        "\n",
        "    # Convert tokens to indices using the TEXT field's vocabulary\n",
        "    custom_text_indices = [dataset.vocab.stoi[token] for token in custom_text]\n",
        "\n",
        "    # Convert to a PyTorch tensor and make a prediction\n",
        "    with torch.no_grad():\n",
        "        if torch.cuda.is_available():\n",
        "            custom_text_tensor = torch.tensor(custom_text_indices, dtype=torch.long).cuda()\n",
        "        else:\n",
        "            custom_text_tensor = torch.tensor(custom_text_indices, dtype=torch.long)\n",
        "        predicted_scores = model(custom_text_tensor.unsqueeze(0))\n",
        "        predicted_label = torch.argmax(predicted_scores, dim=1).item()\n",
        "\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vtxxv6FHxd8C",
        "outputId": "11120228-7c33-4dce-b199-b69ba70ae645"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a text to classify (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Sample data\n",
        "sample_train_articles = [\"Artificial Intelligence (AI) has become indispensable across industries, enhancing decision-making, automating tasks, and driving innovation. In healthcare, AI aids diagnosis, while in business, it personalizes marketing. Its versatile applications underscore its significance in today's world.\",\n",
        "                         \"Machine learning empowers businesses by analyzing data for insights, optimizing operations, and enhancing customer experiences. Leveraging predictive algorithms, companies can make informed decisions, streamline processes, and gain a competitive edge.\",\n",
        "                         \"Work is evolving with remote collaboration, automation, and a flexible workforce. The future promises diverse work arrangements, with technology reshaping job roles and demanding adaptability.\",\n",
        "                         \"The metaverse offers immersive digital experiences, creating new business prospects. Virtual reality, digital economies, and unique marketing channels will redefine commerce.\",\n",
        "                         \"Climate change poses economic risks, impacting industries from agriculture to insurance. Urgent sustainability measures are crucial for mitigating economic disruptions.\",\n",
        "                         \"Renewable energy, led by solar and wind, is revolutionizing global power generation. Clean, sustainable sources are reducing dependence on fossil fuels.\",\n",
        "                         \"Innovations in food production and distribution are critical for ensuring global food security. Sustainable practices and technology-driven solutions will address this challenge.\",\n",
        "                         \"Advanced transportation technologies, like autonomous vehicles and high-speed transit, are poised to revolutionize mobility, enhancing efficiency and sustainability.\",\n",
        "                         \"Technology-driven healthcare is reshaping patient care through telemedicine, AI diagnostics, and personalized treatments. Patient-centric solutions are at the forefront of this transformation.\",\n",
        "                         \"Technology is redefining education with online learning, personalized curricula, and lifelong learning opportunities. Accessible, adaptable learning methods are becoming the norm.\"]\n",
        "\n",
        "sample_train_titles = [\"The Importance of Artificial Intelligence in the Modern World\",\n",
        "         \"How to Use Machine Learning to Improve Your Business\",\n",
        "         \"The Future of Work: What Does It Look Like?\",\n",
        "         \"The Rise of the Metaverse: What It Means for Business\",\n",
        "         \"The Impact of Climate Change on the Economy\",\n",
        "         \"The Future of Energy: How Renewables Will Power Our Planet\",\n",
        "         \"The Future of Food: How We Can Feed the World\",\n",
        "         \"The Future of Transportation: How We Can Move People and Goods More Efficiently\",\n",
        "         \"The Future of Healthcare: How Technology Will Transform Medicine\",\n",
        "         \"The Future of Education: How Technology Will Change the Way We Learn\"]\n",
        "\n",
        "# Tokenize and preprocess the data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sample_train_articles)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in sample_train_articles:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = np.array(y)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "model.fit(X, y, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "id": "lLX6T8mExiG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda419d0-01b1-4be8-839f-fdfb3214ee27"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 3s 31ms/step - loss: 5.1831 - accuracy: 0.0174\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 5.1673 - accuracy: 0.0609\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 5.1413 - accuracy: 0.0696\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 5.0780 - accuracy: 0.0565\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 5.0077 - accuracy: 0.0565\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 4.9538 - accuracy: 0.0565\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 4.8886 - accuracy: 0.0565\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 4.8339 - accuracy: 0.0565\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 4.7868 - accuracy: 0.0696\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 4.7284 - accuracy: 0.0652\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 1s 95ms/step - loss: 4.6387 - accuracy: 0.0739\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 4.5723 - accuracy: 0.0783\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 1s 86ms/step - loss: 4.4727 - accuracy: 0.0783\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 1s 103ms/step - loss: 4.3828 - accuracy: 0.0826\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 4.2836 - accuracy: 0.1043\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 4.1830 - accuracy: 0.1130\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 60ms/step - loss: 4.0796 - accuracy: 0.1130\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 3.9885 - accuracy: 0.1174\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.8685 - accuracy: 0.1261\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 59ms/step - loss: 3.7709 - accuracy: 0.1261\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 3.6755 - accuracy: 0.1348\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 3.5876 - accuracy: 0.1391\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 3.4753 - accuracy: 0.1391\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 3.3700 - accuracy: 0.1522\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 3.3111 - accuracy: 0.1696\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 57ms/step - loss: 3.2320 - accuracy: 0.1478\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 53ms/step - loss: 3.1305 - accuracy: 0.1870\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 3.0476 - accuracy: 0.2043\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 63ms/step - loss: 2.9607 - accuracy: 0.1870\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 1s 61ms/step - loss: 2.8791 - accuracy: 0.2043\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.8040 - accuracy: 0.2304\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.7351 - accuracy: 0.2652\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 2.6777 - accuracy: 0.2870\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.6294 - accuracy: 0.3261\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 2.5617 - accuracy: 0.2913\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 1s 98ms/step - loss: 2.5121 - accuracy: 0.3391\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 1s 112ms/step - loss: 2.4481 - accuracy: 0.3696\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 1s 85ms/step - loss: 2.3931 - accuracy: 0.3652\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 2.3492 - accuracy: 0.3609\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 2.2890 - accuracy: 0.4043\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 2.2270 - accuracy: 0.4261\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 2.1830 - accuracy: 0.4391\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 2.1371 - accuracy: 0.4913\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 2.0943 - accuracy: 0.4826\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 2.0442 - accuracy: 0.5217\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 2.0078 - accuracy: 0.5174\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.9762 - accuracy: 0.5348\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.9424 - accuracy: 0.5522\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.9512 - accuracy: 0.4304\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.9178 - accuracy: 0.5130\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.8278 - accuracy: 0.5957\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.7978 - accuracy: 0.6087\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 1.7695 - accuracy: 0.6261\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.7389 - accuracy: 0.6174\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.7119 - accuracy: 0.6435\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.6594 - accuracy: 0.7000\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 1.6283 - accuracy: 0.7261\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.5946 - accuracy: 0.7435\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.5658 - accuracy: 0.7522\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.5405 - accuracy: 0.7348\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.5077 - accuracy: 0.7652\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.4837 - accuracy: 0.7783\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.4563 - accuracy: 0.7957\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 1.4331 - accuracy: 0.7565\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 1.4657 - accuracy: 0.7000\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 1.4102 - accuracy: 0.7783\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 55ms/step - loss: 1.3965 - accuracy: 0.7565\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 52ms/step - loss: 1.3542 - accuracy: 0.7913\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 54ms/step - loss: 1.3320 - accuracy: 0.8174\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 54ms/step - loss: 1.3418 - accuracy: 0.7913\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 55ms/step - loss: 1.3225 - accuracy: 0.8000\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 53ms/step - loss: 1.2608 - accuracy: 0.8696\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.2300 - accuracy: 0.8435\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.2133 - accuracy: 0.8783\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 1.1846 - accuracy: 0.9000\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.1575 - accuracy: 0.9087\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.1391 - accuracy: 0.9043\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.1137 - accuracy: 0.9217\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.1084 - accuracy: 0.9000\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 1.0742 - accuracy: 0.9391\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 1.0629 - accuracy: 0.9217\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 1.0533 - accuracy: 0.9261\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 1.0256 - accuracy: 0.9217\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.0096 - accuracy: 0.9304\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.0072 - accuracy: 0.9000\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.0052 - accuracy: 0.8870\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.9651 - accuracy: 0.9087\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.9607 - accuracy: 0.9087\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.9516 - accuracy: 0.9391\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.9404 - accuracy: 0.9391\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.9172 - accuracy: 0.9174\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.8832 - accuracy: 0.9522\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.8682 - accuracy: 0.9522\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.8458 - accuracy: 0.9565\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.8265 - accuracy: 0.9565\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.8132 - accuracy: 0.9739\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.7897 - accuracy: 0.9783\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.7788 - accuracy: 0.9783\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.7679 - accuracy: 0.9652\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.7564 - accuracy: 0.9826\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f34b01411e0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the title\n",
        "def generate_title(text):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "    predicted_probs = model.predict(token_list, verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probs)\n",
        "\n",
        "    # Convert the predicted index back to a word\n",
        "    predicted_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_index:\n",
        "            predicted_word = word\n",
        "            break\n",
        "    return predicted_word\n",
        "\n",
        "# Test the model\n",
        "test_article = \"Innovations in food production and distribution are critical for ensuring global food security. Sustainable practices and technology-driven solutions will address this challenge.\"\n",
        "predicted_title = generate_title(test_article)\n",
        "print(\"Predicted Title:\", predicted_title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1CRResgi4Rb",
        "outputId": "0fb0633b-0b95-4c9e-e8f8-04377be8e694"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Title: challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2pgrAOOLjS8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}